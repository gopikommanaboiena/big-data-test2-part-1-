{"cells":[{"cell_type":"code","execution_count":1,"id":"894a6072","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"894a6072","outputId":"b22c9597-b2ae-4126-d0dd-04b19f9bdba1","executionInfo":{"status":"ok","timestamp":1670514378389,"user_tz":300,"elapsed":12060,"user":{"displayName":"Gopi Kommanaboiena","userId":"04768642560532994866"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (4.13.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n"]}],"source":["!pip install gym"]},{"cell_type":"code","execution_count":2,"id":"a08a5f3f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a08a5f3f","outputId":"245eb618-2ef1-4714-bbb7-73412b197646","executionInfo":{"status":"ok","timestamp":1670514406901,"user_tz":300,"elapsed":28519,"user":{"displayName":"Gopi Kommanaboiena","userId":"04768642560532994866"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gym[accept-rom-license,atari]==0.21.0\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 19.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.21.6)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.5.0)\n","Collecting ale-py~=0.7.1\n","  Downloading ale_py-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 6.9 MB/s \n","\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (5.10.0)\n","Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (4.13.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (4.64.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.23.0)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (3.11.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (1.24.3)\n","Building wheels for collected packages: gym, AutoROM.accept-rom-license\n","  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=f074357dac33ca6d37ccea2ce4543fd6d78978ac38dac0135bdfff48fc6960e2\n","  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441028 sha256=4091aa05455f6f5169c84dd75a6a659b521195888863e4528ad6d868de75ba37\n","  Stored in directory: /root/.cache/pip/wheels/51/08/c5/28b973078691a3f8baf99fcaec1ed8f0e05ef6e54d2390212c\n","Successfully built gym AutoROM.accept-rom-license\n","Installing collected packages: AutoROM.accept-rom-license, autorom, gym, ale-py\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.25.2\n","    Uninstalling gym-0.25.2:\n","      Successfully uninstalled gym-0.25.2\n","Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2 gym-0.21.0\n"]}],"source":["!pip install gym[atari,accept-rom-license]==0.21.0"]},{"cell_type":"code","execution_count":3,"id":"adb33377","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adb33377","outputId":"2f81fb06-9679-4acd-d1ec-570cd8f87c4d","executionInfo":{"status":"ok","timestamp":1670514419997,"user_tz":300,"elapsed":13115,"user":{"displayName":"Gopi Kommanaboiena","userId":"04768642560532994866"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-2ba7aadc98e7>:40: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  return I.astype(np.float).ravel()\n"]},{"output_type":"stream","name":"stdout","text":["ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: 1.0 !!!!!!!!\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: -1.0\n","ep 0: game finished, reward: 1.0 !!!!!!!!\n","ep 0: game finished, reward: -1.0\n","resetting env. episode reward total was -19.0. running mean: -19.0\n","Time difference for model is 12.31063417 seconds:\n"]}],"source":["\"\"\" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. \"\"\"\n","import numpy as np\n","import pickle\n","import gym\n","import time\n","from datetime import timedelta\n","start_time=time.monotonic()\n","\n","# hyperparameters\n","H = 600 # number of hidden layer neurons\n","batch_size = 10 # every how many episodes to do a param update?\n","learning_rate = 1e-4\n","gamma = 0.99 # discount factor for reward\n","decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n","resume = False # resume from previous checkpoint?\n","render = False\n","\n","# model initialization\n","D = 80 * 80 # input dimensionality: 80x80 grid\n","if resume:\n","    model = pickle.load(open('save.p', 'rb'))\n","else:\n","    model = {}\n","    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n","    model['W2'] = np.random.randn(H) / np.sqrt(H)\n","\n","grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n","rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n","\n","def sigmoid(x):\n","    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n","\n","def prepro(I):\n","    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n","    I = I[35:195] # crop\n","    I = I[::2,::2,0] # downsample by factor of 2\n","    I[I == 144] = 0 # erase background (background type 1)\n","    I[I == 109] = 0 # erase background (background type 2)\n","    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n","    return I.astype(np.float).ravel()\n","\n","def discount_rewards(r):\n","    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n","    discounted_r = np.zeros_like(r)\n","    running_add = 0\n","    for t in reversed(range(0, r.size)):\n","        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n","        running_add = running_add * gamma + r[t]\n","        discounted_r[t] = running_add\n","    return discounted_r\n","\n","def policy_forward(x):\n","    h = np.dot(model['W1'], x)\n","    h[h<0] = 0 # ReLU nonlinearity\n","    logp = np.dot(model['W2'], h)\n","    p = sigmoid(logp)\n","    return p, h # return probability of taking action 2, and hidden state\n","\n","def policy_backward(eph, epdlogp):\n","    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n","    dW2 = np.dot(eph.T, epdlogp).ravel()\n","    dh = np.outer(epdlogp, model['W2'])\n","    dh[eph <= 0] = 0 # backpro prelu\n","    dW1 = np.dot(dh.T, epx)\n","    return {'W1':dW1, 'W2':dW2}\n","\n","env = gym.make(\"Pong-v0\")\n","observation = env.reset()\n","prev_x = None # used in computing the difference frame\n","xs,hs,dlogps,drs = [],[],[],[]\n","running_reward = None\n","reward_sum = 0\n","episode_number = 0\n","while True:\n","    if render: env.render()\n","\n","  # preprocess the observation, set input to network to be difference image\n","    cur_x = prepro(observation)\n","    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","    prev_x = cur_x\n","\n","  # forward the policy network and sample an action from the returned probability\n","    aprob, h = policy_forward(x)\n","    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n","\n","  # record various intermediates (needed later for backprop)\n","    xs.append(x) # observation\n","    hs.append(h) # hidden state\n","    y = 1 if action == 2 else 0 # a \"fake label\"\n","    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n","\n","  # step the environment and get new measurements\n","    observation, reward, done, info = env.step(action)\n","    reward_sum += reward\n","\n","    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n","\n","    if done: # an episode finished\n","        episode_number += 1\n","\n","    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n","        epx = np.vstack(xs)\n","        eph = np.vstack(hs)\n","        epdlogp = np.vstack(dlogps)\n","        epr = np.vstack(drs)\n","        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n","\n","    # compute the discounted reward backwards through time\n","        discounted_epr = discount_rewards(epr)\n","    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n","        discounted_epr -= np.mean(discounted_epr)\n","        discounted_epr /= np.std(discounted_epr)\n","\n","        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n","        grad = policy_backward(eph, epdlogp)\n","        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n","\n","    # perform rmsprop parameter update every batch_size episodes\n","        if episode_number % batch_size == 0:\n","            for k,v in model.items():\n","                g = grad_buffer[k] # gradient\n","                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n","                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n","                grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n","\n","    # boring book-keeping\n","        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n","        print('resetting env. episode reward total was {}. running mean: {}'.format(reward_sum, running_reward))\n","        if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n","        reward_sum = 0\n","        observation = env.reset() # reset env\n","        prev_x = None\n","        if running_reward>=-19:\n","            break\n","    if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n","        print ('ep {}: game finished, reward: {}'.format(episode_number, reward) + ('' if reward == -1 else ' !!!!!!!!'))\n","end_time=time.monotonic()\n","td_model=end_time-start_time\n","print('Time difference for model is %s seconds:' %(end_time - start_time))\n"]},{"cell_type":"code","execution_count":3,"id":"c7bb7ce3","metadata":{"id":"c7bb7ce3","executionInfo":{"status":"ok","timestamp":1670514419998,"user_tz":300,"elapsed":5,"user":{"displayName":"Gopi Kommanaboiena","userId":"04768642560532994866"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[{"file_id":"1BjlD1cH1RBnGpCN_GCEHOKBcDJ7EyX17","timestamp":1670514947192}]}},"nbformat":4,"nbformat_minor":5}